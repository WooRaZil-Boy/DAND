{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1\n",
    "----\n",
    "\n",
    "# Parsing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'BPI Certification': 'Gold',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': 'Platinum',\n",
       "  'Released': '22 March 1963',\n",
       "  'Title': 'Please Please Me',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '-'},\n",
       " {'BPI Certification': 'Platinum',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': 'Gold',\n",
       "  'Released': '22 November 1963',\n",
       "  'Title': 'With the Beatles',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '-'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '25 November 1963',\n",
       "  'Title': 'Beatlemania! With the Beatles',\n",
       "  'UK Chart Position': '-',\n",
       "  'US Chart Position': '-'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Vee-Jay(US)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '10 January 1964',\n",
       "  'Title': 'Introducing... The Beatles',\n",
       "  'UK Chart Position': '-',\n",
       "  'US Chart Position': '2'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '5xPlatinum',\n",
       "  'Released': '20 January 1964',\n",
       "  'Title': 'Meet the Beatles!',\n",
       "  'UK Chart Position': '-',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '3 February 1964',\n",
       "  'Title': 'Twist and Shout',\n",
       "  'UK Chart Position': '-',\n",
       "  'US Chart Position': '-'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(US)',\n",
       "  'RIAA Certification': '2xPlatinum',\n",
       "  'Released': '10 April 1964',\n",
       "  'Title': \"The Beatles' Second Album\",\n",
       "  'UK Chart Position': '-',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'Capitol(CAN)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '11 May 1964',\n",
       "  'Title': \"The Beatles' Long Tall Sally\",\n",
       "  'UK Chart Position': '-',\n",
       "  'US Chart Position': '-'},\n",
       " {'BPI Certification': '',\n",
       "  'Label': 'United Artists(US)[C]',\n",
       "  'RIAA Certification': '4xPlatinum',\n",
       "  'Released': '26 June 1964',\n",
       "  'Title': \"A Hard Day's Night\",\n",
       "  'UK Chart Position': '-',\n",
       "  'US Chart Position': '1'},\n",
       " {'BPI Certification': 'Gold',\n",
       "  'Label': 'Parlophone(UK)',\n",
       "  'RIAA Certification': '',\n",
       "  'Released': '10 July 1964',\n",
       "  'Title': '',\n",
       "  'UK Chart Position': '1',\n",
       "  'US Chart Position': '-'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"beatles-diskography.csv\"\n",
    "\n",
    "def parse_file(datafile):\n",
    "    data = []\n",
    "    with open(datafile, \"r\") as f:\n",
    "        header = f.readline().split(\",\") #readline : 한 줄 읽고 다음 줄로 넘어간다. split으로 나누면 list로 받는다.\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            if counter == 10:\n",
    "                break\n",
    "            fields = line.split(\",\")\n",
    "            entry = {}\n",
    "            \n",
    "            for i, value in enumerate(fields):\n",
    "                entry[header[i].strip()] = value.strip() #Dic으로 추가\n",
    "                \n",
    "            data.append(entry) #반환할 List에 추가\n",
    "            counter += 1\n",
    "            \n",
    "    return data\n",
    "\n",
    "parse_file(DATAFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # a simple test of your implemetation\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    d = parse_file(datafile)\n",
    "    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n",
    "    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n",
    "\n",
    "    # assert d[0] == firstline\n",
    "    # assert d[9] == tenthline\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CSV Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Title', 'Please Please Me'), ('Released', '22 March 1963'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Gold'), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', 'With the Beatles'), ('Released', '22 November 1963'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Platinum'), ('RIAA Certification', 'Gold')])\n",
      "OrderedDict([('Title', 'Beatlemania! With the Beatles'), ('Released', '25 November 1963'), ('Label', 'Capitol(CAN)'), ('UK Chart Position', '-'), ('US Chart Position', '-'), ('BPI Certification', ''), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', 'Introducing... The Beatles'), ('Released', '10 January 1964'), ('Label', 'Vee-Jay(US)'), ('UK Chart Position', '-'), ('US Chart Position', '2'), ('BPI Certification', ''), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', 'Meet the Beatles!'), ('Released', '20 January 1964'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '5xPlatinum')])\n",
      "OrderedDict([('Title', 'Twist and Shout'), ('Released', '3 February 1964'), ('Label', 'Capitol(CAN)'), ('UK Chart Position', '-'), ('US Chart Position', '-'), ('BPI Certification', ''), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', \"The Beatles' Second Album\"), ('Released', '10 April 1964'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '2xPlatinum')])\n",
      "OrderedDict([('Title', \"The Beatles' Long Tall Sally\"), ('Released', '11 May 1964'), ('Label', 'Capitol(CAN)'), ('UK Chart Position', '-'), ('US Chart Position', '-'), ('BPI Certification', ''), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', \"A Hard Day's Night\"), ('Released', '26 June 1964'), ('Label', 'United Artists(US)[C]'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '4xPlatinum')])\n",
      "OrderedDict([('Title', ''), ('Released', '10 July 1964'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Gold'), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', 'Something New'), ('Released', '20 July 1964'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '2'), ('BPI Certification', ''), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', 'Beatles for Sale'), ('Released', '4 December 1964'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Gold'), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', \"Beatles '65\"), ('Released', '15 December 1964'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '3xPlatinum')])\n",
      "OrderedDict([('Title', 'Beatles VI'), ('Released', '14 June 1965'), ('Label', 'Parlophone(NZ), Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', 'Help!'), ('Released', '6 August 1965'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', ''), ('Released', '13 August 1965'), ('Label', 'Capitol(US)[C]'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '3xPlatinum')])\n",
      "OrderedDict([('Title', 'Rubber Soul'), ('Released', '3 December 1965'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', ''), ('Released', '6 December 1965'), ('Label', 'Capitol(US)[C]'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '6xPlatinum')])\n",
      "OrderedDict([('Title', 'Yesterday and Today'), ('Released', '15 June 1966'), ('Label', 'Capitol(US)'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '2xPlatinum')])\n",
      "OrderedDict([('Title', 'Revolver'), ('Released', '5 August 1966'), ('Label', 'Parlophone(UK)'), ('UK Chart Position', '1'), ('US Chart Position', '-'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '')])\n",
      "OrderedDict([('Title', ''), ('Released', '8 August 1966'), ('Label', 'Capitol(US)[C]'), ('UK Chart Position', '-'), ('US Chart Position', '1'), ('BPI Certification', ''), ('RIAA Certification', '5xPlatinum')])\n",
      "OrderedDict([('Title', \"Sgt. Pepper's Lonely Hearts Club Band\"), ('Released', '1 June 1967'), ('Label', 'Parlophone(UK), Capitol(US)'), ('UK Chart Position', '1'), ('US Chart Position', '1'), ('BPI Certification', '3xPlatinum'), ('RIAA Certification', '11xPlatinum')])\n",
      "OrderedDict([('Title', 'Magical Mystery Tour'), ('Released', '27 November 1967'), ('Label', 'Parlophone(UK), Capitol(US)'), ('UK Chart Position', '31[D]'), ('US Chart Position', '1'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '6xPlatinum')])\n",
      "OrderedDict([('Title', 'The Beatles'), ('Released', '22 November 1968'), ('Label', 'Apple(UK), Capitol(US)'), ('UK Chart Position', '1'), ('US Chart Position', '1'), ('BPI Certification', 'Platinum'), ('RIAA Certification', '19xPlatinum')])\n",
      "OrderedDict([('Title', 'Yellow Submarine'), ('Released', '13 January 1969'), ('Label', 'Apple(UK), Capitol(US)'), ('UK Chart Position', '3'), ('US Chart Position', '2'), ('BPI Certification', 'Silver'), ('RIAA Certification', 'Platinum')])\n",
      "OrderedDict([('Title', 'Abbey Road'), ('Released', '26 September 1969'), ('Label', 'Apple(UK), Capitol(US)'), ('UK Chart Position', '1'), ('US Chart Position', '1'), ('BPI Certification', '2xPlatinum'), ('RIAA Certification', '12xPlatinum')])\n",
      "OrderedDict([('Title', 'Let It Be'), ('Released', '8 May 1970'), ('Label', 'Apple(UK),United Artists(US)'), ('UK Chart Position', '1'), ('US Chart Position', '1'), ('BPI Certification', 'Gold'), ('RIAA Certification', '4xPlatinum')])\n",
      "[OrderedDict([('Title', 'Please Please Me'),\n",
      "              ('Released', '22 March 1963'),\n",
      "              ('Label', 'Parlophone(UK)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', 'Gold'),\n",
      "              ('RIAA Certification', 'Platinum')]),\n",
      " OrderedDict([('Title', 'With the Beatles'),\n",
      "              ('Released', '22 November 1963'),\n",
      "              ('Label', 'Parlophone(UK)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', 'Platinum'),\n",
      "              ('RIAA Certification', 'Gold')]),\n",
      " OrderedDict([('Title', 'Beatlemania! With the Beatles'),\n",
      "              ('Released', '25 November 1963'),\n",
      "              ('Label', 'Capitol(CAN)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '')]),\n",
      " OrderedDict([('Title', 'Introducing... The Beatles'),\n",
      "              ('Released', '10 January 1964'),\n",
      "              ('Label', 'Vee-Jay(US)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '2'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '')]),\n",
      " OrderedDict([('Title', 'Meet the Beatles!'),\n",
      "              ('Released', '20 January 1964'),\n",
      "              ('Label', 'Capitol(US)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '5xPlatinum')]),\n",
      " OrderedDict([('Title', 'Twist and Shout'),\n",
      "              ('Released', '3 February 1964'),\n",
      "              ('Label', 'Capitol(CAN)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '')]),\n",
      " OrderedDict([('Title', \"The Beatles' Second Album\"),\n",
      "              ('Released', '10 April 1964'),\n",
      "              ('Label', 'Capitol(US)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '2xPlatinum')]),\n",
      " OrderedDict([('Title', \"The Beatles' Long Tall Sally\"),\n",
      "              ('Released', '11 May 1964'),\n",
      "              ('Label', 'Capitol(CAN)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '')]),\n",
      " OrderedDict([('Title', \"A Hard Day's Night\"),\n",
      "              ('Released', '26 June 1964'),\n",
      "              ('Label', 'United Artists(US)[C]'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '4xPlatinum')]),\n",
      " OrderedDict([('Title', ''),\n",
      "              ('Released', '10 July 1964'),\n",
      "              ('Label', 'Parlophone(UK)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', 'Gold'),\n",
      "              ('RIAA Certification', '')]),\n",
      " OrderedDict([('Title', 'Something New'),\n",
      "              ('Released', '20 July 1964'),\n",
      "              ('Label', 'Capitol(US)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '2'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', 'Platinum')]),\n",
      " OrderedDict([('Title', 'Beatles for Sale'),\n",
      "              ('Released', '4 December 1964'),\n",
      "              ('Label', 'Parlophone(UK)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', 'Gold'),\n",
      "              ('RIAA Certification', 'Platinum')]),\n",
      " OrderedDict([('Title', \"Beatles '65\"),\n",
      "              ('Released', '15 December 1964'),\n",
      "              ('Label', 'Capitol(US)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '3xPlatinum')]),\n",
      " OrderedDict([('Title', 'Beatles VI'),\n",
      "              ('Released', '14 June 1965'),\n",
      "              ('Label', 'Parlophone(NZ), Capitol(US)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', 'Platinum')]),\n",
      " OrderedDict([('Title', 'Help!'),\n",
      "              ('Released', '6 August 1965'),\n",
      "              ('Label', 'Parlophone(UK)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', 'Platinum'),\n",
      "              ('RIAA Certification', '')]),\n",
      " OrderedDict([('Title', ''),\n",
      "              ('Released', '13 August 1965'),\n",
      "              ('Label', 'Capitol(US)[C]'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '3xPlatinum')]),\n",
      " OrderedDict([('Title', 'Rubber Soul'),\n",
      "              ('Released', '3 December 1965'),\n",
      "              ('Label', 'Parlophone(UK)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', 'Platinum'),\n",
      "              ('RIAA Certification', '')]),\n",
      " OrderedDict([('Title', ''),\n",
      "              ('Released', '6 December 1965'),\n",
      "              ('Label', 'Capitol(US)[C]'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '6xPlatinum')]),\n",
      " OrderedDict([('Title', 'Yesterday and Today'),\n",
      "              ('Released', '15 June 1966'),\n",
      "              ('Label', 'Capitol(US)'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '2xPlatinum')]),\n",
      " OrderedDict([('Title', 'Revolver'),\n",
      "              ('Released', '5 August 1966'),\n",
      "              ('Label', 'Parlophone(UK)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '-'),\n",
      "              ('BPI Certification', 'Platinum'),\n",
      "              ('RIAA Certification', '')]),\n",
      " OrderedDict([('Title', ''),\n",
      "              ('Released', '8 August 1966'),\n",
      "              ('Label', 'Capitol(US)[C]'),\n",
      "              ('UK Chart Position', '-'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', ''),\n",
      "              ('RIAA Certification', '5xPlatinum')]),\n",
      " OrderedDict([('Title', \"Sgt. Pepper's Lonely Hearts Club Band\"),\n",
      "              ('Released', '1 June 1967'),\n",
      "              ('Label', 'Parlophone(UK), Capitol(US)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', '3xPlatinum'),\n",
      "              ('RIAA Certification', '11xPlatinum')]),\n",
      " OrderedDict([('Title', 'Magical Mystery Tour'),\n",
      "              ('Released', '27 November 1967'),\n",
      "              ('Label', 'Parlophone(UK), Capitol(US)'),\n",
      "              ('UK Chart Position', '31[D]'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', 'Platinum'),\n",
      "              ('RIAA Certification', '6xPlatinum')]),\n",
      " OrderedDict([('Title', 'The Beatles'),\n",
      "              ('Released', '22 November 1968'),\n",
      "              ('Label', 'Apple(UK), Capitol(US)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', 'Platinum'),\n",
      "              ('RIAA Certification', '19xPlatinum')]),\n",
      " OrderedDict([('Title', 'Yellow Submarine'),\n",
      "              ('Released', '13 January 1969'),\n",
      "              ('Label', 'Apple(UK), Capitol(US)'),\n",
      "              ('UK Chart Position', '3'),\n",
      "              ('US Chart Position', '2'),\n",
      "              ('BPI Certification', 'Silver'),\n",
      "              ('RIAA Certification', 'Platinum')]),\n",
      " OrderedDict([('Title', 'Abbey Road'),\n",
      "              ('Released', '26 September 1969'),\n",
      "              ('Label', 'Apple(UK), Capitol(US)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', '2xPlatinum'),\n",
      "              ('RIAA Certification', '12xPlatinum')]),\n",
      " OrderedDict([('Title', 'Let It Be'),\n",
      "              ('Released', '8 May 1970'),\n",
      "              ('Label', 'Apple(UK),United Artists(US)'),\n",
      "              ('UK Chart Position', '1'),\n",
      "              ('US Chart Position', '1'),\n",
      "              ('BPI Certification', 'Gold'),\n",
      "              ('RIAA Certification', '4xPlatinum')])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Please Please Me'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "import csv\n",
    "\n",
    "def parse_csv(datafile):\n",
    "    data = []\n",
    "    n = 0\n",
    "    \n",
    "    with open(datafile, \"rt\") as sd:\n",
    "        r = csv.DictReader(sd) #csv.DictReader로 Dic 형태로 불러 온다.\n",
    "        \n",
    "        for line in r:\n",
    "            print(line)\n",
    "            data.append(line)\n",
    "    \n",
    "    return data\n",
    "\n",
    "datafile = os.path.join(DATADIR, DATAFILE)\n",
    "d = parse_csv(DATAFILE)\n",
    "pprint.pprint(d)\n",
    "\n",
    "d[0][\"Title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to XLRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List Comprehension\n",
      "data[3][2]: 1036.0886969999988\n",
      "\n",
      "Cells in a nested loop:\n",
      "sheet.cell_value(row, col) 41277.083333333336\n",
      "sheet.cell_value(row, col) 9238.737309999968\n",
      "sheet.cell_value(row, col) 1438.2052799999994\n",
      "sheet.cell_value(row, col) 1565.4428559999976\n",
      "sheet.cell_value(row, col) 916.7083480000003\n",
      "sheet.cell_value(row, col) 14010.903488000036\n",
      "sheet.cell_value(row, col) 3027.9833399999993\n",
      "sheet.cell_value(row, col) 6165.211119000006\n",
      "sheet.cell_value(row, col) 1157.7416630000007\n",
      "sheet.cell_value(row, col) 37520.93340400001\n",
      "\n",
      "ROWS, COLUMNS, and CELLS:\n",
      "Number of rows in the sheet: 7296\n",
      "Type of data in cell (row 3, col 2): 2\n",
      "Value in cell (row 3, col 2): 1036.0886969999988\n",
      "Get a slice of values in column 3, from rows 1-3: [1411.7505669999982, 1403.4722870000019, 1395.053150000001]\n",
      "\n",
      "DATES:\n",
      "Type of data in cell (row 1, col 0): 3\n",
      "Time in Excel format: 41275.041666666664\n",
      "Convert time to a Python datetime tuple, from the Excel float: (2013, 1, 1, 1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0) #시트 선택\n",
    "\n",
    "    data = [[sheet.cell_value(r, col) #cell_value : r, c 인덱스로 데이터 불러 온다.\n",
    "                for col in range(sheet.ncols)]  #ncols : 열 수, nrows : 행 수 \n",
    "                    for r in range(sheet.nrows)] #모든 데이터 불러서 저장\n",
    "\n",
    "    print(\"\\nList Comprehension\")\n",
    "    print(\"data[3][2]:\", data[3][2])\n",
    "\n",
    "    print(\"\\nCells in a nested loop:\")    \n",
    "    for row in range(sheet.nrows):\n",
    "        for col in range(sheet.ncols):\n",
    "            if row == 50:\n",
    "                print(\"sheet.cell_value(row, col)\", sheet.cell_value(row, col))\n",
    "\n",
    "\n",
    "    ### other useful methods:\n",
    "    print(\"\\nROWS, COLUMNS, and CELLS:\")\n",
    "    print(\"Number of rows in the sheet:\", sheet.nrows)\n",
    "    print(\"Type of data in cell (row 3, col 2):\", sheet.cell_type(3, 2))\n",
    "    print(\"Value in cell (row 3, col 2):\", sheet.cell_value(3, 2))\n",
    "    print(\"Get a slice of values in column 3, from rows 1-3:\", sheet.col_values(3, start_rowx=1, end_rowx=4))\n",
    "    \n",
    "#     Type symbol\tType number\tPython value\n",
    "#     XL_CELL_EMPTY\t0\tempty string u''\n",
    "#     XL_CELL_TEXT\t1\ta Unicode string\n",
    "#     XL_CELL_NUMBER\t2\tfloat\n",
    "#     XL_CELL_DATE\t3\tfloat\n",
    "#     XL_CELL_BOOLEAN\t4\tint; 1 means TRUE, 0 means FALSE\n",
    "#     XL_CELL_ERROR\t5\tint representing internal Excel codes; for a text representation, refer to the supplied dictionary error_text_from_code\n",
    "#     XL_CELL_BLANK\t6\tempty string u''. Note: this type will appear only when open_workbook(..., formatting_info=True) is used.\n",
    "\n",
    "    print(\"\\nDATES:\")\n",
    "    print(\"Type of data in cell (row 1, col 0):\", sheet.cell_type(1, 0))\n",
    "    exceltime = sheet.cell_value(1, 0)\n",
    "          \n",
    "    print(\"Time in Excel format:\", exceltime)\n",
    "    print(\"Convert time to a Python datetime tuple, from the Excel float:\", xlrd.xldate_as_tuple(exceltime, 0)) \n",
    "    #시간 데이터 변환\n",
    "\n",
    "    return data\n",
    "\n",
    "data = parse_file(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'maxtime': (2013, 8, 13, 17, 0, 0), 'maxvalue': 18779.025510000003, 'mintime': (2013, 2, 3, 4, 0, 0), 'minvalue': 6602.113898999982, 'avgcoast': 10976.933460679751}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    coast_data = []\n",
    "    \n",
    "    for i in range(1, sheet.nrows):\n",
    "        coast_data.append(sheet.cell_value(i, 1))\n",
    "    #coast_data = sheet.col_values(1, start_rowx=1, end_rowx=None))으로 할 수도 있다. : 인덱스로 열의 값을 가져온다.\n",
    "\n",
    "    min_id = min(enumerate(coast_data), key=lambda p: p[1])[0] + 1 #최소값 인덱스 구해서 +1 (cv.index()로도 구할 수 있다.)\n",
    "    max_id = max(enumerate(coast_data), key=lambda p: p[1])[0] + 1 #최대값 인덱스 구해서 +1\n",
    "\n",
    "    ### example on how you can get the data\n",
    "    #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "\n",
    "    ### other useful methods:\n",
    "    # print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    # print \"Number of rows in the sheet:\", \n",
    "    # print sheet.nrows\n",
    "    # print \"Type of data in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_type(3, 2)\n",
    "    # print \"Value in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_value(3, 2)\n",
    "    # print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    # print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    # exceltime = sheet.cell_value(1, 0)\n",
    "    # print \"Time in Excel format:\",\n",
    "    # print exceltime\n",
    "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "            'maxtime': xlrd.xldate_as_tuple(sheet.cell_value(max_id, 0), 0),\n",
    "            'maxvalue': max(coast_data),\n",
    "            'mintime': xlrd.xldate_as_tuple(sheet.cell_value(min_id, 0), 0),\n",
    "            'minvalue': min(coast_data),\n",
    "            'avgcoast': sum(coast_data) / len(coast_data)\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "#     open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    print(data)\n",
    "\n",
    "    assert(data['maxtime'] == (2013, 8, 13, 17, 0, 0))\n",
    "    assert(round(data['maxvalue'], 10) == round(18779.02551, 10))\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "{\n",
      "    \"artists\": [\n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null,\n",
      "                    \"end-date\": null,\n",
      "                    \"locale\": null,\n",
      "                    \"name\": \"Nirvana US\",\n",
      "                    \"primary\": null,\n",
      "                    \"sort-name\": \"Nirvana US\",\n",
      "                    \"type\": null\n",
      "                }\n",
      "            ],\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\",\n",
      "                \"name\": \"United States\",\n",
      "                \"sort-name\": \"United States\"\n",
      "            },\n",
      "            \"begin-area\": {\n",
      "                \"id\": \"a640b45c-c173-49b1-8030-973603e895b5\",\n",
      "                \"name\": \"Aberdeen\",\n",
      "                \"sort-name\": \"Aberdeen\"\n",
      "            },\n",
      "            \"country\": \"US\",\n",
      "            \"disambiguation\": \"90s US grunge band\",\n",
      "            \"id\": \"5b11f4ce-a62d-471e-81fc-a69a8278c7da\",\n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1988-01\",\n",
      "                \"end\": \"1994-04-05\",\n",
      "                \"ended\": true\n",
      "            },\n",
      "            \"name\": \"Nirvana\",\n",
      "            \"score\": \"100\",\n",
      "            \"sort-name\": \"Nirvana\",\n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 9,\n",
      "                    \"name\": \"rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 4,\n",
      "                    \"name\": \"alternative rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"90s\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"punk\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 5,\n",
      "                    \"name\": \"american\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"seattle\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 14,\n",
      "                    \"name\": \"grunge\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"band\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"usa\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"alternative\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"am\\u00e9ricain\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"legendary\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"acoustic rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"noise rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"90\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"northwest\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"rock and indie\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"united states\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"nirvana\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 0,\n",
      "                    \"name\": \"kurt cobain\"\n",
      "                }\n",
      "            ],\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\",\n",
      "                \"name\": \"United Kingdom\",\n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            },\n",
      "            \"begin-area\": {\n",
      "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\",\n",
      "                \"name\": \"London\",\n",
      "                \"sort-name\": \"London\"\n",
      "            },\n",
      "            \"country\": \"GB\",\n",
      "            \"disambiguation\": \"60s band from the UK\",\n",
      "            \"id\": \"9282c8b4-ca0b-4c6b-b7e3-4f7762dfc4d6\",\n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1967\",\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana\",\n",
      "            \"score\": \"100\",\n",
      "            \"sort-name\": \"Nirvana\",\n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"pop\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"progressive rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"orchestral\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"british\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"power pop\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"psychedelic rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"soft rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"symphonic rock\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"english\"\n",
      "                }\n",
      "            ],\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"6a264f94-6ff1-30b1-9a81-41f7bfabd616\",\n",
      "                \"name\": \"Finland\",\n",
      "                \"sort-name\": \"Finland\"\n",
      "            },\n",
      "            \"country\": \"FI\",\n",
      "            \"disambiguation\": \"Early 1980's Finnish punk band\",\n",
      "            \"id\": \"85af0709-95db-4fbc-801a-120e9f4766d0\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana\",\n",
      "            \"score\": \"100\",\n",
      "            \"sort-name\": \"Nirvana\",\n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"punk\"\n",
      "                },\n",
      "                {\n",
      "                    \"count\": 1,\n",
      "                    \"name\": \"finland\"\n",
      "                }\n",
      "            ],\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"disambiguation\": \"founded in 1987 by a Michael Jackson double/imitator\",\n",
      "            \"id\": \"3aa878c0-224b-41e5-abd1-63be359d2bca\",\n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1987\",\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana\",\n",
      "            \"score\": \"100\",\n",
      "            \"sort-name\": \"Nirvana\"\n",
      "        },\n",
      "        {\n",
      "            \"disambiguation\": \"French band from Martigues, activ during the 70s.\",\n",
      "            \"id\": \"c49d69dc-e008-47cf-b5ff-160fafb1fe1f\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana\",\n",
      "            \"score\": \"100\",\n",
      "            \"sort-name\": \"Nirvana\"\n",
      "        },\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\",\n",
      "                \"name\": \"United States\",\n",
      "                \"sort-name\": \"United States\"\n",
      "            },\n",
      "            \"country\": \"US\",\n",
      "            \"id\": \"c3a64a25-251b-4d03-afba-1471440245b8\",\n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2009\",\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Approaching Nirvana\",\n",
      "            \"score\": \"62\",\n",
      "            \"sort-name\": \"Approaching Nirvana\",\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null,\n",
      "                    \"end-date\": null,\n",
      "                    \"locale\": null,\n",
      "                    \"name\": \"Nirvana\",\n",
      "                    \"primary\": null,\n",
      "                    \"sort-name\": \"Nirvana\",\n",
      "                    \"type\": null\n",
      "                },\n",
      "                {\n",
      "                    \"begin-date\": null,\n",
      "                    \"end-date\": null,\n",
      "                    \"locale\": null,\n",
      "                    \"name\": \"Prophet 2002\",\n",
      "                    \"primary\": null,\n",
      "                    \"sort-name\": \"Prophet 2002\",\n",
      "                    \"type\": null\n",
      "                }\n",
      "            ],\n",
      "            \"area\": {\n",
      "                \"id\": \"23d10872-f5ae-3f0c-bf55-332788a16ecb\",\n",
      "                \"name\": \"Sweden\",\n",
      "                \"sort-name\": \"Sweden\"\n",
      "            },\n",
      "            \"country\": \"SE\",\n",
      "            \"disambiguation\": \"Swedish death metal band\",\n",
      "            \"id\": \"f2dfdff9-3862-4be0-bf85-9c833fa3059e\",\n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1988\",\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana 2002\",\n",
      "            \"score\": \"62\",\n",
      "            \"sort-name\": \"Nirvana 2002\",\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"329c04ae-3b73-4ca3-996f-75608ab1befb\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana Singh\",\n",
      "            \"score\": \"62\",\n",
      "            \"sort-name\": \"Singh, Nirvana\",\n",
      "            \"type\": \"Person\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"b305320e-c158-43f4-b5be-4450e2f99a32\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"El Nirvana\",\n",
      "            \"score\": \"62\",\n",
      "            \"sort-name\": \"Nirvana, El\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"86f9ae24-ba2a-4d55-9275-0b89b85f6e3a\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Weed Nirvana\",\n",
      "            \"score\": \"62\",\n",
      "            \"sort-name\": \"Weed Nirvana\"\n",
      "        },\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\",\n",
      "                \"name\": \"United States\",\n",
      "                \"sort-name\": \"United States\"\n",
      "            },\n",
      "            \"country\": \"US\",\n",
      "            \"gender\": \"female\",\n",
      "            \"id\": \"206419e0-3a7a-49ce-8437-4e757767d02b\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana Savoury\",\n",
      "            \"score\": \"62\",\n",
      "            \"sort-name\": \"Savoury, Nirvana\",\n",
      "            \"type\": \"Person\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"f58febd3-18de-4371-8d95-4a68d4f79456\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana Kelly\",\n",
      "            \"score\": \"62\",\n",
      "            \"sort-name\": \"Kelly, Nirvana\",\n",
      "            \"type\": \"Person\"\n",
      "        },\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"c920948b-83e3-40b7-8fe9-9ab5abaac55b\",\n",
      "                \"name\": \"Houston\",\n",
      "                \"sort-name\": \"Houston\"\n",
      "            },\n",
      "            \"begin-area\": {\n",
      "                \"id\": \"c920948b-83e3-40b7-8fe9-9ab5abaac55b\",\n",
      "                \"name\": \"Houston\",\n",
      "                \"sort-name\": \"Houston\"\n",
      "            },\n",
      "            \"disambiguation\": \"Nirvana Cover Band\",\n",
      "            \"id\": \"45eacd92-6857-4faa-9283-023e72a1d4b1\",\n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2012\",\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"The Nirvana Experience\",\n",
      "            \"score\": \"50\",\n",
      "            \"sort-name\": \"The Nirvana Experience\",\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"e43ad11b-5d29-45ae-90f7-73ac47fb815d\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"smells like nirvana\",\n",
      "            \"score\": \"50\",\n",
      "            \"sort-name\": \"smells like nirvana\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"bb94730d-22c2-422d-a0a7-fe16a5b3e429\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"The Attainment of Nirvana\",\n",
      "            \"score\": \"50\",\n",
      "            \"sort-name\": \"Attainment of Nirvana, The\",\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"c621114d-73cc-4832-8afe-f13dc261e5af\",\n",
      "                \"name\": \"Gatineau\",\n",
      "                \"sort-name\": \"Gatineau\"\n",
      "            },\n",
      "            \"begin-area\": {\n",
      "                \"id\": \"c621114d-73cc-4832-8afe-f13dc261e5af\",\n",
      "                \"name\": \"Gatineau\",\n",
      "                \"sort-name\": \"Gatineau\"\n",
      "            },\n",
      "            \"id\": \"02c4e6bb-7b7a-4686-8c23-df01bfd42b0e\",\n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2012-04-05\",\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Sappy Nirvana Tribute\",\n",
      "            \"score\": \"50\",\n",
      "            \"sort-name\": \"Sappy Nirvana Tribute\",\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"e8ad73e9-9e7f-41c4-a395-6e29260ff1df\",\n",
      "                \"name\": \"Graz\",\n",
      "                \"sort-name\": \"Graz\"\n",
      "            },\n",
      "            \"begin-area\": {\n",
      "                \"id\": \"e8ad73e9-9e7f-41c4-a395-6e29260ff1df\",\n",
      "                \"name\": \"Graz\",\n",
      "                \"sort-name\": \"Graz\"\n",
      "            },\n",
      "            \"disambiguation\": \"Nirvana-Coverband\",\n",
      "            \"id\": \"46d8dae4-abec-438b-9c62-a3dbb2aaa1b7\",\n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2000\",\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana Teen Spirit\",\n",
      "            \"score\": \"50\",\n",
      "            \"sort-name\": \"Nirvana Teen Spirit\",\n",
      "            \"type\": \"Group\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"e1388435-f80d-434a-9980-f1c9f5aa9b90\",\n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            },\n",
      "            \"name\": \"Nirvana Sitar & String Group\",\n",
      "            \"score\": \"43\",\n",
      "            \"sort-name\": \"Nirvana Sitar & String Group\"\n",
      "        }\n",
      "    ],\n",
      "    \"count\": 18,\n",
      "    \"created\": \"2017-08-24T22:59:38.141Z\",\n",
      "    \"offset\": 0\n",
      "}\n",
      "\n",
      "ARTIST:\n",
      "{\n",
      "    \"disambiguation\": \"founded in 1987 by a Michael Jackson double/imitator\",\n",
      "    \"id\": \"3aa878c0-224b-41e5-abd1-63be359d2bca\",\n",
      "    \"life-span\": {\n",
      "        \"begin\": \"1987\",\n",
      "        \"ended\": null\n",
      "    },\n",
      "    \"name\": \"Nirvana\",\n",
      "    \"score\": \"100\",\n",
      "    \"sort-name\": \"Nirvana\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/3aa878c0-224b-41e5-abd1-63be359d2bca?inc=releases&fmt=json\n",
      "\n",
      "ONE RELEASE:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-bbfc6c765255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-94-bbfc6c765255>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Print information about releases from the selected band\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nONE RELEASE:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreleases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mrelease_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreleases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To experiment with this code freely you will have to run this code locally.\n",
    "Take a look at the main() function for an example of how to use the code. We\n",
    "have provided example json output in the other code editor tabs for you to look\n",
    "at, but you will not be able to run any queries through our UI.\n",
    "\"\"\"\n",
    "import json\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    \"\"\"\n",
    "    This is the main function for making queries to the musicbrainz API. The\n",
    "    query should return a json document.\n",
    "    \"\"\"\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print(\"requesting\", r.url)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    \"\"\"\n",
    "    This adds an artist name to the query parameters before making an API call\n",
    "    to the function above.\n",
    "    \"\"\"\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    \"\"\"\n",
    "    After we get our output, we can use this function to format it to be more\n",
    "    readable.\n",
    "    \"\"\"\n",
    "    if type(data) == dict:\n",
    "        print(json.dumps(data, indent=indent, sort_keys=True))\n",
    "    else:\n",
    "        print(data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Below is an example investigation to help you get started in your\n",
    "    exploration. Modify the function calls and indexing below to answer the\n",
    "    questions on the next quiz.\n",
    "\n",
    "    HINT: Note how the output we get from the site is a multi-level JSON\n",
    "    document, so try making print statements to step through the structure one\n",
    "    level at a time or copy the output to a separate output file. Experimenting\n",
    "    and iteration will be key to understand the structure of the data!\n",
    "    \"\"\"\n",
    "\n",
    "    # Query for information in the database about bands named Nirvana\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    # Isolate information from the 4th band returned (index 3)\n",
    "    print(\"\\nARTIST:\")\n",
    "    pretty_print(results[\"artists\"][3])\n",
    "\n",
    "    # Query for releases from that band using the artist_id\n",
    "    artist_id = results[\"artists\"][3][\"id\"]\n",
    "    artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    releases = artist_data[\"releases\"]\n",
    "\n",
    "    # Print information about releases from the selected band\n",
    "    print(\"\\nONE RELEASE:\")\n",
    "    pretty_print(releases[0], indent=2)\n",
    "\n",
    "    release_titles = [r[\"title\"] for r in releases]\n",
    "    print(\"\\nALL TITLES:\")\n",
    "    for t in release_titles:\n",
    "        print(t)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3Afirst+aid+kit&fmt=json\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"first aid kit\")\n",
    "results = results[\"artists\"]\n",
    "count = 0\n",
    "\n",
    "for dic in results:\n",
    "    if dic[\"name\"].lower() == \"first aid kit\":\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3AQUEEN&fmt=json\n",
      "{'id': 'f03d09b3-39dc-4083-afd6-159e3f0d462f', 'name': 'London', 'sort-name': 'London'}\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "{'id': 'fb665c49-69f0-4f29-9f30-1e0ac08f0ecc', 'name': 'Paramaribo', 'sort-name': 'Paramaribo'}\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "{'id': 'acae38b3-e987-404e-b4db-c29ad3323c70', 'name': 'Gorizia', 'sort-name': 'Gorizia'}\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "{'id': '85c7cd5f-6fe2-4195-a44d-69fa390bd6ec', 'name': 'Newark', 'sort-name': 'Newark'}\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"QUEEN\")\n",
    "results = results[\"artists\"]\n",
    "\n",
    "for dic in results:\n",
    "    print(dic.get(\"begin-area\")) #begin-area가 없는 데이터들도 있다. get()으로 가져오면 없을 경우에도 오류 나지 않고 None 반환\n",
    "\n",
    "# results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ABeatles&fmt=json\n",
      "Los Beatles\n"
     ]
    }
   ],
   "source": [
    "results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Beatles\")\n",
    "results = results[\"artists\"]\n",
    "\n",
    "for dic in results:\n",
    "    if dic.get(\"aliases\") is not None:\n",
    "        locale = dic.get('aliases')\n",
    "        \n",
    "        for sub_dic in locale:\n",
    "            if sub_dic.get(\"locale\") == \"es\":\n",
    "                print(sub_dic.get(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3ANirvana&fmt=json\n",
      "90s US grunge band\n",
      "60s band from the UK\n",
      "Early 1980's Finnish punk band\n",
      "founded in 1987 by a Michael Jackson double/imitator\n",
      "French band from Martigues, activ during the 70s.\n",
      "None\n",
      "Swedish death metal band\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Nirvana Cover Band\n",
      "None\n",
      "None\n",
      "None\n",
      "Nirvana-Coverband\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"Nirvana\")\n",
    "results = results[\"artists\"]\n",
    "\n",
    "for dic in results:\n",
    "    print(dic.get(\"disambiguation\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2\n",
    "----\n",
    "\n",
    "# Using CSV Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'rt') as f:\n",
    "        csv_data = csv.reader(f) #파일 불러 오기. list로 가져온다\n",
    "        #csv.DictReader : 파이썬의 Dict의 객체 형태로 파일을 읽어온다. 첫번째 줄에 column명들이 명시가 되어 있어야 한다.\n",
    "        \n",
    "        name_data = next(csv_data) #파이썬 내부 함수 next()로 iterator에서 한 단계씩 넘어간다. 넘어간 열 반환. loop 돌 필요 없이 데이터 뭉쳐 있을 때.\n",
    "        name = name_data[1]\n",
    "        \n",
    "        next(csv_data)\n",
    "        \n",
    "        data = [[cell for cell in row_data] for row_data in csv_data]\n",
    "    # Do not change the line below\n",
    "    return (name, data)\n",
    "\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    assert(name == \"MOUNTAIN VIEW MOFFETT FLD NAS\")\n",
    "    assert(data[0][1] == \"01:00\")\n",
    "    assert(data[2][0] == \"01/01/2005\")\n",
    "    assert(data[2][5] == \"2\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile) #엑셀 파일 열기\n",
    "    sheet = workbook.sheet_by_index(0) #시트 지정\n",
    "    xl_data = [[sheet.cell_value(r, col) #cell_value : r, c 인덱스로 데이터 불러 온다.\n",
    "                for col in range(sheet.ncols)]  #ncols : 열 수, nrows : 행 수 \n",
    "                    for r in range(sheet.nrows)] #모든 데이터 불러서 저장\n",
    "    \n",
    "    column_title_list = xl_data[0]\n",
    "    column_value_list = [[xl_data[i][ii] for i in range(1, sheet.nrows)] for ii in range(sheet.ncols)]\n",
    "    \n",
    "    data = []\n",
    "    data.append([\"Station\", \"Year\", \"Month\", \"Day\", \"Hour\", \"Max Load\"])\n",
    "    \n",
    "    for i in range(1, len(column_title_list)-1):\n",
    "        title = column_title_list[i]\n",
    "        max_id, max_value = max(enumerate(column_value_list[i]), key=lambda p: p[1]) #최대값 #cv.index()로도 구할 수 있다.\n",
    "        max_time = xlrd.xldate_as_tuple(column_value_list[0][max_id], 0)\n",
    "        \n",
    "        data.append([title, max_time[0], max_time[1], max_time[2], max_time[3], max_value])\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    with open(filename, 'w', newline='') as f: #쓰기 모드\n",
    "        writer = csv.writer(f, delimiter='|') #파일 쓰기\n",
    "        writer.writerows(data) #row 추가\n",
    "    return\n",
    "\n",
    "    \n",
    "def test():\n",
    "#     open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert(max_answer == max_line)\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert(ans[station][field] == line[field])\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert(number_of_rows == 8)\n",
    "\n",
    "        # Check Station Names\n",
    "        assert(set(stations) == set(correct_stations))\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result. (See the file \n",
    "'popular-viewed-1.json' from the tabs above.)\n",
    "\n",
    "Your task is to modify the article_overview() function to process the saved\n",
    "file that represents the most popular articles (by view count) from the last\n",
    "day, and return a tuple of variables containing the following data:\n",
    "- labels: list of dictionaries, where the keys are the \"section\" values and\n",
    "  values are the \"title\" values for each of the retrieved articles.\n",
    "- urls: list of URLs for all 'media' entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview() function. See the test() \n",
    "function for examples of the elements of the output lists.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself.\n",
    "\"\"\"\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"\",\n",
    "            \"article\": \"\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls = []\n",
    "\n",
    "    for article in data:\n",
    "        section = article[\"section\"]\n",
    "        title = article[\"title\"]\n",
    "        titles.append({section: title})\n",
    "        if \"media\" in article:\n",
    "            for m in article[\"media\"]:\n",
    "                for mm in m[\"media-metadata\"]:\n",
    "                    if mm[\"format\"] == \"Standard Thumbnail\":\n",
    "                        urls.append(mm[\"url\"])\n",
    "                \n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return (titles, urls)\n",
    "\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print(\"You need to register for NYTimes Developer account to run this program.\")\n",
    "        print(\"See Intructor notes for information\")\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print(\"Time period can be 1,7, 30 days only\")\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print(\"kind can be only one of viewed/shared/emailed\")\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "\n",
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    print(len(urls))\n",
    "    \n",
    "    assert(len(titles) == 20)\n",
    "    assert(len(urls) == 30)\n",
    "    assert(titles[2] == {'Opinion': 'Professors, We Need You!'})\n",
    "    assert(urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3\n",
    "----\n",
    "## XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xml.etree.ElementTree.ElementTree object at 0x1070dba58>\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys, but you have to extract the attributes from the \"insr\" tag\n",
    "# and add them to the list for the dictionary key \"insr\"\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname) #xml 불러와서\n",
    "    return tree.getroot() #루트 반환\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'): #태그 순 fm/bibl/aug/au\n",
    "        data = { #fm/bibl/aug/au 밑의 fnm, snm, email, insr 태그들\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data[\"fnm\"] = author.find('./fnm').text #xml 태그 안의 텍스트 불러 와 딕셔너리에 저장\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text\n",
    "        insr = author.findall('./insr') #insr에는 여러 값이 있을 수 있으므로 findall로. \n",
    "        \n",
    "        for i in insr:\n",
    "            data[\"insr\"].append(i.attrib[\"iid\"]) #attrib로 태그 속성에 접근. iid로 값 불러오기.\n",
    "            \n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert(data[0] == solution[0])\n",
    "    assert(data[1][\"insr\"] == solution[1][\"insr\"])\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"page_source.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html: #html파일 불러온다.\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\") #BeautifulSoup 라이브러리로 불러온다. lxml 파서 이용. \n",
    "        #전체 html파일을 그대로 가져온다.\n",
    "        \n",
    "        ev = soup.find(id=\"__EVENTVALIDATION\") #id로 찾는다.\n",
    "        data[\"eventvalidation\"] = ev[\"value\"] #id로 찾아온 객체의 value 저장\n",
    "\n",
    "        vs = soup.find(id=\"__VIEWSTATE\")\n",
    "        data[\"viewstate\"] = vs[\"value\"]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data): #직접 인터넷 파일을 불러올 경우.\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert(data[\"eventvalidation\"] != \"\")\n",
    "    assert(data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\"))\n",
    "    assert(data[\"viewstate\"].startswith(\"/wEPDwUKLTI\"))\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4\n",
    "----\n",
    "\n",
    "## Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id=\"CarrierList\")\n",
    "\n",
    "        for carrier in carrier_list.findAll(\"option\"): #한 개가 아니기 때문에 findAll로 찾아야 한다.\n",
    "            if not carrier[\"value\"].startswith(\"All\"):\n",
    "                data.append(carrier[\"value\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert(len(data) == 16)\n",
    "    assert(\"FL\" in data)\n",
    "    assert(\"NK\" in data)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        airport_list = soup.find(id=\"AirportList\")\n",
    "        \n",
    "        for option in airport_list.findAll(\"option\"):\n",
    "            if not option[\"value\"].startswith(\"All\"):\n",
    "                data.append(option[\"value\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert(len(data) == 15)\n",
    "    assert(\"ATL\" in data)\n",
    "    assert(\"ABR\" in data)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a simple test...\n",
      "... success!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    del(files[0])\n",
    "    \n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        table = soup.find(id=\"DataGrid1\")\n",
    "        \n",
    "        for tr in table.findAll(\"tr\", {\"class\": \"dataTDRight\"}): #class가 dataTDRight인 tr요소 찾기            \n",
    "            td = tr.findAll(\"td\")\n",
    "            values = []\n",
    "            \n",
    "            if td[1].text == \"TOTAL\":\n",
    "                continue\n",
    "                \n",
    "            for value in td:\n",
    "                values.append(int(value.text.replace(',',''))) #, 없애고 int형으로 리스트에 삽입\n",
    "            \n",
    "            data.append({\"courier\": \"FL\",\n",
    "                        \"airport\": \"ATL\",\n",
    "                        \"year\": values[0],\n",
    "                        \"month\": values[1],\n",
    "                        \"flights\": {\"domestic\": values[2],\n",
    "                        \"international\":values[3]}\n",
    "                        })\n",
    "            \n",
    "                \n",
    "#             td = tr.findNext(\"td\")\n",
    "            \n",
    "#             for td in tr:\n",
    "#                 td_dict = {}\n",
    "#                 if not isinstance(td, bs4.element.NavigableString): #이 클래스에 속하는 지 아닌 지 확인\n",
    "#                     print(td.text)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Running a simple test...\")\n",
    "#     open_zip(datadir)\n",
    "    files = process_all(datadir)\n",
    "    data = []\n",
    "    # Test will loop over three data files.\n",
    "    for f in files:\n",
    "        data += process_file(f)\n",
    "        \n",
    "#     assert(len(data) == 399)  # Total number of rows\n",
    "#     for entry in data[:3]:\n",
    "#         assert(type(entry[\"year\"]) == int)\n",
    "#         assert(type(entry[\"month\"]) == int)\n",
    "#         assert(type(entry[\"flights\"][\"domestic\"]) == int)\n",
    "#         assert(len(entry[\"airport\"]) == 3)\n",
    "#         assert(len(entry[\"courier\"]) == 2)\n",
    "#     assert(data[0][\"courier\"] == 'FL')\n",
    "#     assert(data[0][\"month\"] == 10)\n",
    "#     assert(data[-1][\"airport\"] == \"ATL\")\n",
    "#     assert(data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425})\n",
    "    \n",
    "    print(\"... success!\")\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "The following things should be done:\n",
    "- check if the field \"productionStartYear\" contains a year\n",
    "- check if the year is in range 1886-2014\n",
    "- convert the value of the field to be just a year (not full datetime)\n",
    "- the rest of the fields and values should stay the same\n",
    "- if the value of the field is a valid year in the range as described above,\n",
    "  write that line to the output_good file\n",
    "- if the value of the field is not a valid year as described above, \n",
    "  write that line to the output_bad file\n",
    "- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "- you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
    "  They will take care of dealing with the header.\n",
    "\n",
    "You can write helper functions for checking the data and writing the files, but we will call only the \n",
    "'process_file' with 3 arguments (inputfile, output_good, output_bad).\n",
    "\"\"\"\n",
    "import csv\n",
    "import pprint\n",
    "\n",
    "INPUT_FILE = 'autos.csv'\n",
    "OUTPUT_GOOD = 'autos-valid.csv'\n",
    "OUTPUT_BAD = 'FIXME-autos.csv'\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "    # store data into lists for output\n",
    "    data_good = []\n",
    "    data_bad = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames\n",
    "        for row in reader:\n",
    "            # validate URI value\n",
    "            if row['URI'].find(\"dbpedia.org\") < 0:\n",
    "                continue\n",
    "\n",
    "            ps_year = row['productionStartYear'][:4]\n",
    "            try: # use try/except to filter valid items\n",
    "                ps_year = int(ps_year)\n",
    "                row['productionStartYear'] = ps_year\n",
    "                if (ps_year >= 1886) and (ps_year <= 2014):\n",
    "                    data_good.append(row)\n",
    "                else:\n",
    "                    data_bad.append(row)\n",
    "            except ValueError: # non-numeric strings caught by exception\n",
    "                if ps_year == 'NULL':\n",
    "                    data_bad.append(row)\n",
    "\n",
    "    # Write processed data to output files\n",
    "    with open(output_good, \"w\") as good:\n",
    "        writer = csv.DictWriter(good, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_good:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    with open(output_bad, \"w\") as bad:\n",
    "        writer = csv.DictWriter(bad, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in data_bad:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def test():\n",
    "    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
